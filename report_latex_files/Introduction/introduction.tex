%%% Thesis Introduction --------------------------------------------------
\chapter{Introduction}
\ifpdf
    \graphicspath{{Introduction/IntroductionFigs/PNG/}{Introduction/IntroductionFigs/PDF/}{Introduction/IntroductionFigs/}}
\else
    \graphicspath{{Introduction/IntroductionFigs/EPS/}{Introduction/IntroductionFigs/}}
\fi

Deep Learning has been a big boon to our society. It has been very effective in so many applications from various domains. It is able to produce state of the art results in image classification including medical images. This level of performance is mainly attributed to the complexity of the model which enables the model to learn intricate details of the models and consequently enables the model to distinguish images of different models. However, the increase in complexity has made it harder for the humans to understand why the model took a certain set of decisions. It may not be guaranteed that the model learnt the right set of features that distinguishes one class of images from another class of images. If the model does not extract the right features, then shortcut learning or wrong generalisation of
a class may occur.

This means understanding the logic behind the decisions taken by a model is of paramount importance in the field of medical imaging. Transparency and trustability of the deep learning models is highly expected in the medical field. The decisions taken in this field can potentially affect the lives of the humans and thus it is important that the models learn the right features. We attempt to build more intepretable medical classification models. Interpretability is defined as the notion of understanding the decisions taken by a model. We try to solve this problem by using two interpretability inducing loss functions which are Class Distinctiveness Loss and Spatial Coherence Loss. Further, we perform more experiments by modifying model backbones, exploring different saliency map generation methods and investigating the possibility of using Multi-Input-Multi-Output (MIMO) Network in our training setup.

We also explored the problem of class imbalance. In almost all medical image datasets, the distribution of images among the classes will not be uniform. Such imbalanced nature of models pushes the models to learn the classes with more samples easily while struggling to learn the classes with less samples. We attempt to overcome this issue by incorporating Focal Loss and Deep AUC Maximization.

Another facet of a model is important in the domain of medical imaging, the model calibration errors. Calibration errors quantify the difference between the true probability, say of an image belonging to a class, and the probability output by a model. If the model doesn't output the precise probability, then again it is an issue in a high-stake application like medical imaging. We analyse how calibrated the models are and further calibrate the models using Temperature Scaling to reduce Expected Calibration Error (ECE).


%%% ----------------------------------------------------------------------


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End: 
